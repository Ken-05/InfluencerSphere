import numpy as np
import pandas as pd
import os
import joblib
from typing import Dict, Any, List

# Define the absolute path to the project's root directory for model loading
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')

class EngagementAgent:
    """
    Agent 3: Predicts the Post-Level Engagement Potential (PLEP) for a given
    post using the full feature vector generated by the Feature Orchestrator.

    Loads the PLEP regression model (e.g., Gradient Boosting Regressor).
    """
    MODEL_FILENAME = 'engagement_predictor_model.joblib'

    def __init__(self):
        self.model = self._load_model()
        self.is_ready = self.model is not None

    def _load_model(self) -> Any:
        """Loads the serialized model artifact from the models directory."""
        model_path = os.path.join(MODEL_DIR, self.MODEL_FILENAME)
        if not os.path.exists(model_path):
            print(f"WARNING: Model artifact not found at {model_path}. Agent will run in mock mode.")
            return None
        try:
            model = joblib.load(model_path)
            print(f"SUCCESS: Agent 3 ({self.MODEL_FILENAME}) loaded.")
            return model
        except Exception as e:
            print(f"ERROR loading model {self.MODEL_FILENAME}: {e}. Running in mock mode.")
            return None

    def _convert_features_to_df(self, features: Dict[str, Any]) -> pd.DataFrame:
        """Converts the feature dictionary into a format expected by the model."""
        # This function ensures alignment and dtype conversion.
        # Using df, let the model handle missing feature warnings.
        df = pd.DataFrame([features])
        return df.select_dtypes(include=[np.number]) # Only pass numerical data to the model

    def _mock_prediction(self, features: Dict[str, Any]) -> float:
        """Provides a simple mock PLEP prediction."""
        # Simple heuristic based on known features
        caption_score = features.get('Caption_Length', 100) / 200.0 * 1.5
        visual_score = features.get('Image_Sharpness_Score', 0.5) * 3.0
        base_rate = 2.5

        mock_plep = base_rate + visual_score - caption_score
        return float(np.clip(mock_plep, 0.5, 10.0))

    def predict(self, input_features: Dict[str, Any]) -> float:
        """
        Predicts the Post-Level Engagement Potential (PLEP) as a percentage.
        """
        if not self.is_ready:
            return self._mock_prediction(input_features)

        try:
            df_input = self._convert_features_to_df(input_features)
            # The prediction is the expected weighted engagement rate (PLEP)
            prediction = self.model.predict(df_input)[0]
            # Clamp the result to a plausible range (e.g., 0% to 15%)
            return float(np.clip(prediction, 0.0, 15.0))
        except Exception as e:
            print(f"Prediction Error in EngagementAgent: {e}. Falling back to mock prediction.")
            return self._mock_prediction(input_features)

    def get_diagnostics(self, input_features: Dict[str, Any]) -> Dict[str, Any]:
        """
        Provides diagnostic insights by analyzing feature importance for the PLEP prediction.
        Crucial for the potential Creator/Influencer Dashboard (Feature Importance Scoring).
        """
        plep_score = self.predict(input_features)

        ###### Later: access self.model.feature_importances_ or SHAP/LIME.
        # Mock the top contributing features based on input values.
        feature_importance_list = []

        # Positive Contributors Mock
        if input_features.get('Image_Sharpness_Score', 0) > 0.7:
            feature_importance_list.append({"name": "Visual Quality (Sharpness)", "impact": "High Positive"})
        if input_features.get('Sentiment_Score', 0) > 0.8:
            feature_importance_list.append({"name": "Caption Sentiment", "impact": "Positive"})

        # Negative Contributors Mock
        if input_features.get('Caption_Length', 100) > 400:
            feature_importance_list.append({"name": "Caption Length", "impact": "Negative (Too Long)"})
        if input_features.get('Niche_Score', 0) < 6.0:
            feature_importance_list.append({"name": "Niche Relevance", "impact": "Negative (Off-Topic)"})

        return {
            "Predicted_PLEP": f"{plep_score:.2f}%",
            "Top_Contributors": feature_importance_list[:3],
            "PLEP_Optimization_Note": "Focus on high-quality visuals and niche-relevant captions."
        }